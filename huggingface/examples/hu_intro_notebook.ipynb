{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n"]}],"source":["from transformers import pipeline\n","generator = pipeline(\"text-generation\")"]},{"cell_type":"markdown","metadata":{},"source":["You should noticed the default model supplied was GPT, though we could change this is the args of the generator creation"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"data":{"text/plain":["[{'generated_text': 'I am learning NLP because I want to learn my voice, to get back to my previous way of studying. It would be much better if I had access to the tools to learn how to perform this, because it involves what I would call \"'}]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["generator(\"I am learning NLP because\")"]},{"cell_type":"markdown","metadata":{},"source":["The results may or may not be decent but Notice the warning that the attention mask and the pad token id were not set</br> The attention mask is used to mask the attention to the padding tokens as you would need if you have inputs that are of diffenrent lengths...remember networks need the same structure and size for the tensor operations. The pad token id is used to pad the input to the model"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["#\n","# you can also load any tokenizer or model that hugginface supports and load those in the pipeline\n","# like this\n","# from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n","# model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n","# from transformers import pipeline\n","\n","# generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["The output is a list of dictionaries.This particular one is a list of one element of one dict key:value. So doing something like this you can get just the generated text. "]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["I am learning NLP because as I have done many times before I started NLP training with a specific training group, one I chose as the standard for my current program. It was one of the best classes they offered me. The rest of the\n"]}],"source":["output = generator(\"I am learning NLP because\")\n","print(output[0][\"generated_text\"])"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["{'generated_text': \"I am learning NLP because of what I read. I do want to work on that. It's a really good starting point. I think everybody agrees what happens when you don't succeed. You don't just get this or that. Just don\"}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.12 ('pytorch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"897f7bf9cef63c96b52358f2746121f57775fe08989758f1ceb409d9a6eb6e06"}}},"nbformat":4,"nbformat_minor":2}
